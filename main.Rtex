%%%%%%%%%%%%%%%%%%%%%%%%
% Compile with XeLaTeX %
%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
  % Packages and settings
  \usepackage{fontspec}
    \setmainfont{Charis SIL}
  \usepackage[style=apa, backend=biber]{biblatex}
    \addbibresource{References.bib}
  \usepackage{hyperref}
    \hypersetup{colorlinks=true, allcolors=blue}
  \usepackage[group-minimum-digits=4, group-separator={,}]{siunitx}
  \usepackage{enumitem}

  % Document information
  \title{Using language models for holistic language variety comparisons\footnote{
    Data and code available at \url{https://osf.io/9cjpw/}.
  }}
  \author{Joshua McNeill}
  \date{\today}

  % New commands
  \newcommand{\orth}[1]{$\langle$#1$\rangle$}
  \newcommand{\lexi}[1]{\textit{#1}}
  \newcommand{\gloss}[1]{`#1'}

\begin{document}
  % <<settings_load_scripts, echo = FALSE>>=
  % read_chunk("analysis.R")
  % opts_chunk$set(echo = FALSE,
  %                warning = FALSE,
  %                message = FALSE,
  %                results = "asis",
  %                fig.height = 3,
  %                fig.width = 5)
  % @
  \maketitle
  \section{Introduction}
    \label{sec:intro}
    One of the goals of scientific research is to discover generalizable facts, and work on language variation is no different.
    Variationists are often interested in describing language varieties and distinguishing between language varieties.
    This task is typically accomplished by analyzing several salient linguistic variables in fine detail and either generalizing to whole varieties from these several variables.
    The focus on deeply analyzing a small number of variables has advanced the understanding of language variation in more ways than can be mentioned here.
    While this paradigm shift from the broad analyses of many variables done dialectology, out of which language variation developed, has been valuable, modern technological advances have made it possible to once again consider analyzing broad swaths of linguistic variables at once.
    The objective for this study is thus to develop a method for holistically quantifying the distance between language varieties in a way that can account for practically all potential linguistic variables.

    Some work on language variation has to already returned to its dialectological roots at least in spirit, and so a look at what has been done recently will help situate the method being proposed here.
    As such, section \ref{sec:intro_saliency} will discuss some advantages and disadvantages of traditional methods of analyzing language variation to show what can be gained from a more holistic approach, and section \ref{sec:intro_clusters} will review the limits of how holistic such traditional studies tend to get.
    Section \ref{sec:intro_nlp} will explain a bit of what is done in the field of natural language processing (NLP), and section \ref{sec:intro_nlpimpact} will review how such technological advances have been applied to variationist studies, leading to the inclusion of many more variables than was previously practical.

    \subsection{Generalizing from saliency}
      \label{sec:intro_saliency}
      In traditional variationist work, generalizations about whole varieties are sometimes drawn from a relatively small set of linguistic variables, perhaps as few as five.
      The assumption underlying that makes this possible is that the variables analyzed are salient and so have social meaning and so are what really distinguish varieties.
      This connection between saliency and social meaning has indeed been acknowledged in variationist literature \parencite[p.~235]{podesva_salience_2011} and has at least been implicit in variationist theory since its foundations.

      \textcite{labov_sociolinguistic_1972} himself introduced the idea of saliency into language variation in proposing the concepts of linguistic variables being either indicators, markers, or stereotypes, the latter two carrying saliency.
      It was later claimed that most linguistic variables are markers \parencite{bell_language_1984}, carrying saliency but not being explicitly commented on by speakers when describing one variety versus another.
      The importance of saliency for classifying types of linguistic variables was also part of the concept of orders of indexicality \parencite{silverstein_indexical_2003}, where second and third order indexicals would be somewhat equivalent to markers and stereotypes, respectively, in terms of saliency.

      Saliency has not only been a consistent aspect of variationist theorizing, but it has proved useful for analyses.
      One example is in analyzing how varieties comes to be social constructs, recognized as existing in the minds of speakers, which has been referred to in language variation as enregisterment \parencite{agha_social_2003}.
      In essence, the process of enregisterment is based on features increasing in saliency.
      This was initially applied to Received Pronunciation \parencite{agha_social_2003} but has also been applied to the development of Pittsburghese based mostly on an analysis of (aw) monophthongization, though some other features are discussed also such as the term \lexi{yinz} \gloss{you (pl.)} \parencite{johnstone_mobility_2006}.
      Additionally, the saliency has been used to distinguish varieties spoken by different groups within a single geographic area in that residents will explicitly describe linguistic features that they associate with each group.
      This was the case in a study of Beijing where two types of Beijingers and one type of non-Beijinger were associated with one linguistic variable each \parencite{zhang_chinese_2005}.\footnote{
        Technically, one group, the ``smooth operators'', was associated with two variables, but each variable involved [É¹] \parencite[pp.~441-444]{zhang_chinese_2005}.
      }

      The sort of explicit commentary noted in \textcite{zhang_chinese_2005} is one way to establish whether a linguistic variable is salient or not.
      To quantify such commentary requires developing indices where perhaps certain types of commentary each count in the index or commentary in general is counts as just one item among other types in the index.
      In fact, saliency has sometimes even been quantified on purely linguistic grounds \parencite[p.~237]{podesva_salience_2011}.\footnote{
        \textcite{podesva_salience_2011} does of course acknowledge social saliency, as well.
      } Perhaps a more common way of identifying saliency has come through experimentation, however.
      Perceptual studies that invovle experimentation fall under paradigms such as eye-tracking \parencite[e.g.,][]{donofrio_persona-based_2015} or matched-guise experiments \parencite[e.g.,][]{campbell-kibler_nature_2009, delforge_nobody_2012}.
      Findings in such studies include showing that preconceptions of Valley Girls and Californians can impact perceptions of the TRAP vowel \parencite{donofrio_persona-based_2015}, that the alveolar variant of (ing) is judged differently depending on the listener \parencite{campbell-kibler_nature_2009}, and that vowel devoicing in Andean Spanish is less noticeable by listeners when they do not want to notice it, as those who denied having the feature still had it at similar rates to those from elsewhere in whom the feature was regularly perceived \parencite{delforge_nobody_2012}.

      The point is that saliency does indeed exist, can be measured, and carries import for analyses, particularly if one is interested in social meaning.
      To the extent that varieties are social constructs, it is also a rather safe assumption to say that salient variables are what distinguish varieties as that which is not socially meaningful could not be a defining feature of a social construct. % Can I get a reference for this?
      However, when looking at varieties more as concrete systems that are coherent, consistent, and objectively different from each other, comparing salienct variables alone may miss even a large number of distinctions that a more holistic comparison could capture.
      Furthermore, being able to compare between varieties both holistically and by looking at only salient variables opens the possibility of finding contrasts that may be enlightening, such as the possibility that two varieties different in many more ways than what is noticeable to speakers, yet the focus in variationist work has mainly been on salient variables alone.

    \subsection{Looking at clusters of linguistic variables}
      \label{sec:intro_clusters}
      Language variation studies have of course examined clusters of variables and systems of interrelated variables.
      The focus in these cases is still linguistic variables that are thought to be salient, though the number of variables analyzed has at times approached a relatively larger scale.
      This is particularly true when the analysis focuses on ``features'', which are effectively treated as linguistic variables that have two realizations: present or absent.

      Studies of African-American English (AAE)\footnote{
        I am including what researchers may refer to as African-American Vernacular English (AAVE) under this term as the distinction is not particularly important for the present study.
      }, perhaps due simply to there being so many, include both analyses of small clusters of variables but also larger sets of features.
      In some cases, there is an apparent relationship between the variables, such as in \textcite{rickford_addressee-_1994} where the analysis included the zero copula, habitual invariant \lexi{be}, the plural \lexi{-s} morpheme, the 3rd person present \lexi{-s} verbal inflection, and the possessive \lexi{-s} morpheme.
      The first two of these both implicate the verb \lexi{to be} whereas the last three all involve homophonous suffixes.
      Such relationships are not always present, however, as in an examination of Martin Luther King, Jr's language that included the zero copula, (ing), (t/d) deletion, (r), word-final consonant cluster reduction, and prosody \parencite{wolfram_significance_2016}.
      If a structural relationship exists between these linguistic variables, it is not evident at first glance.

      When sets of features from AAE are studied, the number of variables grows substantially.
      In the case of \textcite{van_hofwegen_coming_2010}, 32 features were tallied in individuals' speech as a way to measure how heavily AAE their speech was.
      The measure itself, along with the chosen features, was developed in research on speech pathology and education and where it is referred to as the dialect density measure (DDM).
      While the proposal of an appellation such as DDM suggests something of a novel approach, similar measures have in fact been used elsewhere, as well.
      In a study looking into the use of AAE features in gay British men on Twitter, 25 AAE features were analyzed according to their frequencies \parencite{ilbury_sassy_2020}.
      Also involving Twitter, over 30 non-standard spellings thought to be associated with AAE were analyzed in order to draw AAE dialect maps in the US \parencite{jones_toward_2015}.
      While these feature-conceptualized studies include far more than the typical five to seven clustered variables, they still fall well short of including all possible variables in these varieties.

      If studies of AAE sometimes include clusters of linguistic variables that appear structurally related, it is not always clear that this structural relationship is what drove the choice of variables.
      In many other cases, there is little question that the researchers are interested in a cluster of variables because of their structural relationship.
      This is particularly evident in examinations of vowel systems or large portions of vowel systems, where there is often though not always a chain shift involved.
      This has been done since the foundational variationist work in New York City \parencite{labov_social_2006} and has continued in studies of other areas such as Belfast \parencite{milroy_language_1987}, King of Prussia \parencite{payne_factors_1980}, Syndey \parencite{horvath_delimiting_1987}, the Detroit area \parencite{eckert_linguistic_2000}, and Philadelphia \parencite{eckert_phonetics_2017}.
      Similarly, though without implicating a chain shift, the majority of the pronoun system of Louisiana French as spoken in Houma has been analyzed as a whole \parencite{rottet_language_1995} as well as chunks of the verbal morphology system and and copula system in Cajun English \parencite{dubois_verbal_2003}.
      Finally, perhaps one of the most explicit treatments of a system of covariation was \citeauthor{guy_cognitive_2013}'s (\citeyear{guy_cognitive_2013}) analysis of Brazilian Portuguese as spoken in Rio de Janeiro which had the explicit goal of examining the existence or non-existence of distinctive sociolects by determining if four linguistic variables covaried.
      While these sort of studies go further towards covering all the variables of particular linguistic systems, there is once again much left to be described if one wishes to compare the distance between varieties in their entireties.
      Furthermore, not all subsystems of varieties are contained in such a relatively small number of variables.
      For instance, it is not possible the lexical system of a variety with five, ten, or even one hundred variables.

    \subsection{Advances in NLP}
      \label{sec:intro_nlp}
      A significant part of what has made it possible to analyze much larger numbers of linguistic variables at once is the continued development of NLP technology.
      Indeed, not only has development in NLP become more and more active, NLP has also made its way into other fields. This includes sociolinguistics, where the resulting interdisciplinary field has become known as computational sociolinguistics \parencite{nguyen_computational_2016}.
      While a thorough discussion of NLP is beyond the scope of the present study, a brief overview of the basics of what can be done and how is relevant to make the holistic method of comparing varieties proposed here more understandable.
      NLP has certainly found its way into language variation, but not every variationist is a computational sociolinguist.

      The building of large corpora is not purely an activity related to NLP, but they are central to the development of NLP tools and sizes have exploded over the last decade.
      Sociolinguistic interviews themselves yield corpora, of course, and the first corpus to be over one million words of written English, the Brown corpus, was collected already in the 1960s \parencite{francis_brown_1964}.
      Efforts to digitize written language have continued through the present day with corpora such as the British National Corpus (BNC) consisting over just under 100 million words \parencite{burnard_reference_2007}, the Corpus of Contemporary American English (COCA) consisting of over 400 million words at its start \parencite{davies_corpus_2010}, and Google Books consisting of over 40 million books \parencite{lee_15_2019}.

      In addition to digitized written language, data mining techniques have made collecting large contemporary corpora from social media platforms so prevalent that there is more of a question about the ethics behind such activities than about how to accomplish the task \parencite{darcy_ethics_2012}.
      Corpora from Twitter, for instance, have reached sizes such as 62 million tweets \parencite[p.~519]{hong_language_2011} or 114 million tweets \parencite[p.~13]{eisenstein_phonological_2013} or even 924 million tweets \parencite[p.~244]{huang_understanding_2016}, each tweet containing up to 140 characters\footnote{
        The character limit for tweets was subsequently raised to 280 characters \parencite{rosen_tweeting_2017}.
      }. Depending on the scope of the sort of data the researcher wishes to collect, such corpora can be constructed in relatively little time, as well, with even the largest example given taking only a year to finish.
      Of course, the extent to which such large corpora present opportunities for linguistic research is limited by the tools available to deal with so much data.
      NLP provides a number of tools that help expedite the sort of analyses that variationists might carry out, including part of speech tagging, sentence parsing, and document classification.

      Corpora have been tagged for parts-of-speech (POS) since at least the development of the Brown corpus, which was done by hand.
      A common NLP task then is to perform POS tagging automatically on new corpora.
      This typiocally involves training a tagger on documents that were first hand-coded for POS, such as the aforementioned Brown corpus.
      The general idea behind training is to construct a dictionary made up of each word that has been seen in the hand-coded corpus with its most likely POS tag.
      This can be as simple as associating the most frequent POS tag for each word to that word but can also get much more complex, perhaps by taking into account the POS of the surrounding words, as well.
      For example, training might lead to listing \lexi{run} as most likely a verb, but if taking into account context, it might also list \lexi{run} preceded by a determiner as a noun.
      There are many possibilities and approaches, but the key point is that hand-coded data is always used as input for the training algorithm.

      Related to POS tagging is the task of parsing sentences, which is also a classic task in NLP.
      This is done in much the same way as POS tagging where some training data is used to train a parser that can then automatically parse new sentences, though naturally the training data must be annotated for no just POS tags but also syntactic structures.
      Fortunately, like the hand-coded POS tags in the Brown corpus, there are today a number of corpora hand-coded with syntactic structures.
      These are referred to as treebanks, perhaps the most well-known example being the Penn Treebank for English \parencite{prasad_penn_2019}.
      Both automatic sentences parsers and POS taggers are useful for variationists looking at syntax but also potentially any other variationist whose linguistic variables may be syntactically conditioned.
      For instance, one may want a POS tagger if one is studying the (ing) variable and suspects it to varying according to lexical category \parencite{houston_continuity_1985}.

      A common NLP task that is particularly relevant to the current study is that of document classification, where a ``document'' is understood as any amount of text that acts as a unit, be it a tweet or a book.
      A classic example of this task is that of identifying the topic of a document.
      The goal in these cases might be to spot spam in e-mail \parencite{dada_machine_2019}, to classify news articles, to mine opinions in text, and so on \parencite{minaee_deep_2021}.
      The general approach to this problem is to take a collection of documents that have already been classified by hand and once again use them as input for a training algorithm in order to train a classifier that can be used on new documents.
      The training algorithm may draw conclusions from the input data such as the use of the word ``senator'' over a certain number of times increases the likelihood that a new article is about politics.
      The result is not entirely different from the dictionaries generated by POS tagging.

      The classification task related to the current study is that of language variety identification, which as it sounds, refers to classifying the language variety in which a document is written.
      There is a related task called language identification, but the former involves varieties that are quite similar and the latter varieties that are quite different.
      Language variety identification is not as old of a task in NLP as some of the others, but work has been done on Arabic varieties during the MADAR workshop \parencite{bouamor_madar_2019}, and Balkan languages, Indonesian versus Malaysian, Portuguese varieties, Spanish varieties, French varieties, and Persian versus Dari during the VarDial workshop \parencite{zampieri_findings_2017}.
      A particularly effective approach to the tasks involved in the VarDial workshop made use of $n$-gram language models (LMs) \parencite{duvenhage_short_2019}.
      An $n$-gram LM is generated from training documents in that the training algorithm makes a list of all the $n$-grams found in the training documents along with their frequencies.
      An $n$-gram itself is some $n$ number of consecutive symbols, which are typically consecutive characters or consecutive words.
      For instance, in sentence \ref{sent:ngram_example} below, the word bigrams are \orth{this is}, \orth{is a}, and \orth{a sentence}, whereas the character 4-grams are \orth{this}, \orth{his }, \orth{is i}, \orth{s is}, \orth{ is }, and so on.
      An $n$-gram LM then would be a list of all the $n$-grams of whichever type the researcher decides to use from all the documents that are known to be in a particular language variety.
      The classifier would then make a decision on the language variety of new texts based on their relationship to the $n$-gram LMs that were constructed.
      In the case of \textcite{duvenhage_short_2019}, he achieved very good results by using a combination of word unigrams and bigrams and character bigrams, 4-grams, and 6-grams which influenced the use of these LM types in the present study, as will be discussed further in section \ref{sec:meths_lms} of the \nameref{sec:meths} section.
      \begin{enumerate}
        \item \label{sent:ngram_example} \orth{this is a sentence}
      \end{enumerate}

      These tools help variationists relegate much work that used to be done by hand to a machine, allowing the scale projects to increase substantially.
      Admittedly, it is still prudent to go through samples of data that has been automatically coded for quality assurance, but doing so is much quicker than coding by hand.
      Additionally, perhaps the biggest yet simplest technological advancement that has aided variationists, particularly when considering how many variables can be analyzed at once, has just been counting.
      A few lines of basic code can return counts for tokens of linguistic variables from millions to even billions of words of text, something that would have taken an enormous amount of time in the early days of the field.

    \subsection{The impact of NLP on language variation research}
      \label{sec:intro_nlpimpact}
      As computational sociolinguistics has developed in recent years, bringing NLP technology into language variation research, studies have begun to analyze many more variables at once to diverse ends.
      Often these studies mine data from Twitter which presents certain advantages such as broad demographic coverage \parencite[p.~5]{wojcik_sizing_2019}, good API access, and fewer ethical grey areas than other platforms as tweets are public by default, accessible even to those without Twitter accounts themselves, which leads users to being less likely to post confidential information \parencite[p.~46]{ehrlich_microblogging_2010}.
      However, the methods employed in large-scale analyses such as those described below are not limited to social media data.

      Research examining very many linguistic variables at once is often focused on regional variation, though objectives as been as diverse as looking at language change, style, and gender.
      \textcite{huang_understanding_2016} studied 211 sets of lexical variables in order to examine regional variation throughout the US as expressed on Twitter.
      Similarly, though not including quite as many variables, \textcite{grieve_mapping_2018} looked at 54 lexical items on Twitter that they judged to be innovations with the goal of mapping their geographic diffusion over time.
      With a very different interest, \textcite{pavalanathan_audience-modulated_2015} put \citeauthor{bell_language_1984}'s (\citeyear{bell_language_1984}) audience design framework for style shifting to the test by analyzing 200 lexical variables on Twitter, treating each variable as a binary where the lexical item is either present or absent, which is not a typical approach to linguistic variables in variationist research where the norm is to analyze manifest tokens of variables, though \citeauthor{pavalanathan_audience-modulated_2015}'s approach is not at all atypical for computational sociolinguistics.

      The impact of gender on variation has also been analyzed on Twitter.
      \textcite{bamman_gender_2014} employed 10,000 lexical items to do so.
      One of their goals was to see which lexical items were associated with specific genders, though they also used these items to train a classifier to identify the gender of Twitter users in the first place as this information is not part of users' profiles nor is it always clear how users identify from cursory glances.
      Their approach of training a classifier involved generating a LM, which is at the heart of the method proposed in the present study to compare language varieties holistically.

      In some ways, these studies harken back to the early days of dialectology where the goal was to produce atlases that demonstrated the boundaries between regional varieties such as the Atlas linguistique de la France for French \parencite{gillieron_atlas_1902} or the Sprach-Atlas for German \parencite{wenker_sprach-atlas_2020}.
      Technologies that are ever-present today but unimaginable in the 19th and early 20th centuries allow new insights to be gained from studies with similar objectives to those early ones that relied on questionnaires alone.
      Indeed, technological advances have always served as catalysts for paradigm changes in methods in science in general but even in sociolinguistics.
      As \textcite{milroy_authority_2012} once remarked, the development and availability of tape recorders is what made sociolinguistic research based on real speech a reality starting in the 1960s (p.~52).

    \subsection{Generalizing from $n$-gram language models}
      \label{sec:intro_lmproposal}
      The aforementioned studies and took advantage of advances in NLP to analyze many more variables at once than was ever done or even possible before, making them much more holistic examinations of varieties than studies based in traditional methods.
      However, it is possible to go further.
      Importantly, all of these large-scale studies have looked at lexical variation alone despite the large number of linguistic variables used, leaving out the other linguistic levels.
      In the case of variation in written language, a truly holistic analysis would aim to capture morphological, syntactic, and even orthographic variation in addition to lexical variation.

      An approach that would allow for comparing language varieties on all linguistic levels would involve using character $n$-gram and word $n$-gram LMs.
      $n$-gram LMs have been used successfully in NLP for the task of language identification and language variety identification, as discussed in section \ref{sec:intro_nlp}.
      Achieving high accuracy in distinguishing betweening two closely related language varieties suggests that similar methods can be used to quantify the distance between two already-identified varieties, as well.
      As LMs are simply probability distributions over some set of symbols, existing statistical metrics meant to quantify the distance between two distributions can be used.
      My research question is thus the following:
      \begin{itemize}
        \item[RQ:] \label{research_question} Are $n$-gram LMs useful for quantifying the linguistic distance between varieties in a more holistic way than was previously possible?
      \end{itemize}

  \section{Methods}
    \label{sec:meths}
    There are three important factors to consider when using LMs for comparing language varieties: which corpus/corpora to use, which LM type(s) to use, and which metric to use to quantify the distance between the LMs.
    These three factors will be considered in sections \ref{sec:meths_data}, \ref{sec:meths_lms}, and \ref{sec:meths_analysis} below, respectively.

    \subsection{Data}
      \label{sec:meths_data}
      The primary interest of this study is to develop baseline results for the method, meaning that the method will be applied to language varieties that are \lexi{a priori} distinct enough that they are socially considered different languages as well as language varieties that are \lexi{a priori} identical for all intents and purposes.
      A natural choice for a corpus given this interest is the Europarl corpus \parencite{koehn_europarl:_2009}.

      The Europarl corpus is a parallel corpus that includes speech from proceedings of the European Union dating back to 1996.
      A parallel corpus aligns text in one language with its translation in one or more other languages.
      In the case of the Europarl corpus, speakers are recorded in their own language and then translated into the other official languages of the European Union, which at the outset of the recording of these proceedings meant ten other languages.

      % Who does the translating at the EU?

      The portions of the corpus used to construct LMs for \lexi{a priori} different languages were relatively simple to choose, but the portions used for identical varieties requires a couple assumptions.
      For different languages, the portions of the corpus considered to be English were compared to the portions considered to be French, meaning LMs for the English texts were compared to LMs for French texts.
      These two languages were chosen primarily because I am very comfortable speaking both myself, allowing for investigation of any anomalies if they may arise that would not be possible for languages that I do not know.
      The decision was not related to how different English and French are from each other compared to any other language pairs.
      Naturally, these languages have a relationship and will thus have some similarities in their models, such as the presence of interlingual homographs such as English \orth{coin} 'coin' and French \orth{coin} 'corner', but that which is important is that they are distinct enough overall that quantifying their distance would show very great distance.
      The greatest possible distance in the case of written language comparisons would be between languages that use different scripts entirely, but a baseline need not show the maximum and minimum distances but rather very large and very small distances that help situate comparisons between similar but different language varieties.

      The portions of the corpus used to construct LMs for the \lexi{a priori} identical varieties came from English and French again.
      However, in this case, the English texts were split in half with LMs constructed for the first half to be compared to LMs constructed for the second half.
      The same was done for the French texts.
      The idea is that the first half of the texts for any given language should yield LMs that are fairly identical to those of the second half of texts from the same language.
      There are of course different speakers represented in the corpus, and it is quite possible that those who spoke early in the corpus are not the same as those who spoke later and that these two groups of speakers use different language varieties.
      This is assumed to not be a great issue due to the corpus representing formal proceedings.
      All speakers are national representatives of their respective countries communicating in an exceptionally formal forum and so are assumed to converge on the same language norms in these texts.

      % [Twitter data will be left out for the QP1 version] Twitter data
        % Characteristics of Twitter
          % What makes this appropriate for this study
        % Tweets collected from Louisiana and filtered for discussion of race and ethnicity
          % Narrowing the type of data analyzed helps ensure that distinctions between communities won't be the result of simply different topics of conversation
          % The race/ethnicity situation in Louisiana has been analyzed before and is complex, so narrowing in this way may provide insights into social questions, though that's not the primary objective of this study
        % Coding
          % The goal here is to compare linguistic distance between communities
          % Community detection
            % Communities are conceptualized on structural grounds in that those who interact with the same people on a consistent basis are considered to form a community
              % In this case, a directed tweet counts as the sender interacting with the receiver
              % The number of directed tweets between two users counts as the amount of interaction
            % Blondel et al.'s community detection algorithm is used
          % Language models will be built for a handful of communities which contain substantial numbers of tweets about race/ethnicity
            % Race/ethnicity are chosen because Louisiana has a complex history in this regard
              % Historically was not divided by Black and White, instead having Black, Creole, and White, but has come to be Black=Creole vs White=Cajun today
            % Race/ethnicity tweets will be detected using a classifier trained on 1% of the total tweets collected (~40,000)
            % There is some subjectivity about what exactly constitutes discussion of race/ethnicity
              % Include some examples of tweets that are unarguably about race/ethnicity and some that are not

      \subsection{Language models}
        \label{sec:meths_lms}
    % Language models
      % Following Duvenhage (2019) due to his success for DSL 17, both word and character n-grams were trained, unigrams and bigrams for the former, bigrams, 4-grams, and 6-grams for the latter.
        % These models each capture different levels of the language, so I conceptualize them as follows:
          % Word unigrams: lexicon
          % Word bigrams: syntax (or at least some small amount of syntax)
          % Character n-grams: orthography and morphology
      \subsection{Analysis}
        \label{sec:meths_analysis}
    % Analysis
      % Two statistics are used: KL divergence and cosine similarity
        % Both of these show essentially the same thing (i.e., how similar/different two distributions are), but the former is easier to interpret and the latter makes comparisons between language pairs easier as values are always between -1 and 1
        % These are calculated for like-models (e.g., word unigram to word unigram, character 4-gram to character 4-gram, etc.)
          % This makes it possible to see how distant two varieties are on different linguistic levels

  \section{Results}
    \label{sec:results}
  % Results
    % europarl baseline results
      % What are the KL divergence and cosine similarity values when comparing French and English?
        % What about when comparing different model types? (Look at combining all models and looking at models individually)
      % What are the KL divergence and cosine similarity values when comparing English-English comparisons and French-French comparisons?
        % What about when comparing different model types? (Look at combining all models and looking at models individually)
    % [After finishing the QP1 version of this,] do the same for the Twitter data between detected communities

  \section{Discussion}
    \label{sec:discuss}
  % Discussion
    % This technique for comparing varieties holistically appears to work, but there are some limitations to consider and questions left unanswered
      % Are the language models used here the right models?
      % How big do corpora need to be in order for this technique to work?
      % How does this relate to traditional work that typically examines only a few linguistic variables but in great detail?
    % Are the models used truly the best language models that could be used here?
      % For instance, KL divergence here is calculated based on the empirical probabilities for each n-gram in each model (i.e., relative frequencies)
      % An alternative would be to use something like term frequency minus inverse document frequency (TF-IDF), which is commonly used in NLP and is meant to give the greatest weight in a model to n-grams that are frequent in a small number of documents
        % Eisenstein (2014) took a similar approach when identifying terms of import for differentiating between dialects on Twitter (5)
      % If, however, the type of models used here are not optimal, the approach can still be just as easily applied to other types of language models, both existing and yet to be proposed types
    % Related to whether these are the best types of language models to use for this technique of comparing varieties is the question of how large the corpora that are used to generate the language models ought to be
      % The europarl corpora are quite large, which presumably means there would not be a great difference between something like an empirical probability and any other sort of calculated probability
      % There are, however, cases where a researcher might want to apply the technique when using much smaller corpora
      % There may very well be a threshold where different language model types are more or less useful depending on whether the corpora size is above or below the threshold
        % This is something that cannot be addressed in the present study, but would be worthwhile to examine in future work
    % As was discussed earlier, salient linguistic variables may be far more important than other variables
      % For instance, the technique used here might find two varieties differ in that one variety uses the preposition "for" where another uses "of" sometimes
        % It is not evident that such a small difference is noticeable at all, even subconsciously, by speakers of these varieties
      % If the differences found between two varieties are not salient on any level, does it actually make sense to speak of them as two separate varieties?
        % This question cannot be answered here, but it is at least a question that can now be examined by comparing the results from a holistic technique as used here to results for the same data from a more traditional, fine-grained analysis

  \section{Conclusion}
    \label{sec:conc}

  \printbibliography
\end{document}
