% Introduction
  % Variationist work has traditionally focused on a handful of bellweather variables and generalized out from there.
    % This is the case despite the more holistic approaches from work in dialectology that came before.
    % Variationist work in CMC has allowed for many more variables to be included at a time, moving back towards a holistic approach.
    % The objective for this study is to develop a method for comparing the languages of communities holistically, including practically all possible variables.
  % Variationist work that makes generalizations to communities from small sets of variables
    % Which group distinctions have been drawn
    % What assumptions remain that call for more holistic analyses
      % At least implicitly, the assumption is that a small number of variables are representative of entire varieties
        % Of course, explicit commentary by community members or perceptual studies may suggest that these small sets of variables are in fact the most salient, but that does not mean that they alone tell us how close or far apart two varieties are
          % In fact, it's been suggested that saliency is a prerequisite before a linguistic variable can have social meaning (Podesva 2011:235)
          % Bell (1984) claimed that most linguistic variables are markers (Labov 1972) (152), but that is not necessarily still the case
          % Stereotypes (Labov 1972) and variables that have obtained 3rd order indexicality (Silverstein 2003) have been proposed, each roughly referring to variables that are salient enough to receive overt commentary, so the notion that variables can be this salient is widely accepted
          % The process of enregisterment of Pittsburghese has been examined largely in terms of the growing salience of variables (Johnstone et al. 2006)
          % [Possibly] give examples of where a small number of salient variables has been useful
            % Differentiating between "smooth operators", "alley saunterers", and "cosmopolitans" in China (Zhang 2005)
          % Indices have sometimes been used to determine saliency (Podesva 2011:237), but explicit commentary and perceptual studies are perhaps the best way, and latter have indeed become common in recent times
            % These studies often take experimental approaches, instead of the traditional observational variationist studies, using eye-tracking (D'Onofrio 2015), matched-guise experiments (Campbell-Kibler 2009; Delforge 2012)
            % Preconceptions of Valley Girls and Californians can impact perceptions of the TRAP vowel (D'Onofrio 2015)
            % The alveolar (ing) variant has been shown to be judged differently depending on the listener, but there was always a judgement (Campbell-Kibler 2009)
            % Vowel devoicing in Andean Spanish has been shown to be less noticeable by listeners when they do not want to notice it, as those who denied having the feature still had it at similar rates to those from elsewhere in whom they regularly perceived the feature (Delforge 2012)
          % The point is that saliency is indeed important when one is interested in social meaning
    % If interested in saliency, being able to compare the distance between varieties holistically with the differences in the use of a few salient variables may still be enlightening, though, and is not currently done
  % Variationist work that has involved looking for clusters of variables or large numbers of variables
    % Clusters
      % AAVE studies may focus on a small number of variables, but sometimes look at large numbers of features
        % Small number of variables
          % Zero copula, habitual be, plural -s, 3rd person present -s, and possessive -s in a speaker from the Bay Area (Rickford & McNair-Knox 1994)
          % Zero copula, (ing), (t/d) deletion, (r), word-final cluster reduction, and prosody in MLK Jr.'s speech (Wolfram et al. 2016)
        % The dialect density measure (DDM), developed in research on speech pathology and education, involves counting the frequency of features and has been employed in the study of variation in AAE using 32 different features (Van Hofwegen & Wolfram 2010)
        % Similarly, a set of 25 AAVE features as used by gay British men on Twitter have been analyzed according to frequency (Ilbury 2020)
        % Also on Twitter, 30+ non-standard spellings thought to be associated with AAVE have been analyzed in order to draw AAVE dialect maps in the US (Jones 2015)
      % Studies that look at systems
        % Vowel systems or at least good chunks of vowel systems
          % This has been done since the foundational variationist work in NYC (Labov 1966) and has continued in studies of other areas such as Belfast (Milroy 1980), King of Prussia (Payne 1980), Sydney (Horvath & Sankoff 1987), the Detroit area (Eckert 2000), Philadelphia (Eckert & Labov 2017)
        % Other systems
          % A large amount of the pronoun system of Louisiana French speakers in Houma (Rottet 1995)
          % Parts of the verbal morphology system and copula system in Cajun English (Dubois & Horvath 2003)
        % The covariation of several variables in Brazilian Portuguese in Rio de Janeiro has been examined to corroborate the existence of distinctive sociolects (Guy 2013)
          % This approaches the idea of drawing comparisons between varieties, but that was not really the goal, hence only four linguistic variables were studied as those were socially stratified (Guy 2013:64-65)
    % Variationist work that brings in many more variables
      % Technological advances have made processing large amounts of data much easier and so has made analyzing many variables at once much easier
        % There is more access to data
          % Not only have corpora created through sociolinguistic interviews been collected since at least the 1960s, large swaths of existing written language have been digitized (e.g., Google Books, BNC, COCA, Brown)
          % Data mining social media has made collecting large contemporary corpora more the question of what's ethical much more prominent than the question of how to obtain data (see D'Arcy & Young 2012 for some focused discussion)
            % Corpora for Twitter, for instance, have reached sizes such as 62 million tweets (Hong et al. 2011:519) or 114 million tweets (Eisenstein 2013:13) or even 924 million tweets (Huang et al. 2016:244)
        % The field of natural language processing (NLP) has led to the development of better and better tools
          % 
      % What sort of variables are used and which group distinctions have been drawn
      % This is fairly holistic, but is it holistic enough?
        % A lot of these studies are really only looking at lexical variation even though they include many variables
  % An even more holistic approach is to use character n-gram and word n-gram language models
    % This has been used in classifiers in NLP for the task of language identification and language variety identification
    % The success of these models in distinguishing between even minorly different language varieties suggests that they can be compared to quantify the linguistic distance between two varieties, as well
      % Language models in NLP are, after all, essentially just probability distributions over some set of symbols
  % Research question
    % Are n-gram language models useful for quantifying the linguistic distance between varieties in a more holistic way than has been previously possible?
% Methods
  % Data collection
    % The europarl corpus
      % This is a parallel corpus that can be used to set the baseline for what one should expect between what are generally viewed as completely different languages and between one variety at different times
        % For different languages: built models from the English and French documents
        % For one variety: built models from the first and second halves of the English documents
        % In effect, this data can be said to be coded for language and text section
    % Twitter data (Twitter data will be left out for the QP1 version)
      % Characteristics of Twitter
        % What makes this appropriate for this study
      % Tweets collected from Louisiana and filtered for discussion of race and ethnicity
        % Narrowing the type of data analyzed helps ensure that distinctions between communities won't be the result of simply different topics of conversation
        % The race/ethnicity situation in Louisiana has been analyzed before and is complex, so narrowing in this way may provide insights into social questions, though that's not the primary objective of this study
      % Coding
        % The goal here is to compare linguistic distance between communities
        % Community detection
          % Communities are conceptualized on structural grounds in that those who interact with the same people on a consistent basis are considered to form a community
            % In this case, a directed tweet counts as the sender interacting with the receiver
            % The number of directed tweets between two users counts as the amount of interaction
          % Blondel et al.'s community detection algorithm is used
        % Language models will be built for a handful of communities which contain substantial numbers of tweets about race/ethnicity
          % Race/ethnicity are chosen because Louisiana has a complex history in this regard
            % Historically was not divided by Black and White, instead having Black, Creole, and White, but has come to be Black=Creole vs White=Cajun today
          % Race/ethnicity tweets will be detected using a classifier trained on 1% of the total tweets collected (~40,000)
          % There is some subjectivity about what exactly constitutes discussion of race/ethnicity
            % Include some examples of tweets that are unarguably about race/ethnicity and some that are not
  % Language models
    % Following Duvenhage (2019) due to his success for DSL 17, both word and character n-grams were trained, unigrams and bigrams for the former, bigrams, 4-grams, and 6-grams for the latter.
      % These models each capture different levels of the language, so I conceptualize them as follows:
        % Word unigrams: lexicon
        % Word bigrams: syntax (or at least some small amount of syntax)
        % Character n-grams: orthography and morphology
  % Analysis
    % Two statistics are used: KL divergence and cosine similarity
      % Both of these show essentially the same thing (i.e., how similar/different two distributions are), but the former is easier to interpret and the latter makes comparisons between language pairs easier as values are always between -1 and 1
      % These are calculated for like-models (e.g., word unigram to word unigram, character 4-gram to character 4-gram, etc.)
        % This makes it possible to see how distant two varieties are on different linguistic levels
% Results
  % europarl baseline results
    % What are the KL divergence and cosine similarity values when comparing French and English?
      % What about when comparing different model types? (Look at combining all models and looking at models individually)
    % What are the KL divergence and cosine similarity values when comparing English-English comparisons and French-French comparisons?
      % What about when comparing different model types? (Look at combining all models and looking at models individually)
  % Do the same for the Twitter data between detected communities
% Discussion
