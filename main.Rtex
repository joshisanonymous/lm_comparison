%%%%%%%%%%%%%%%%%%%%%%%%
% Compile with XeLaTeX %
%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
  % Packages and settings
  \usepackage{fontspec}
    \setmainfont{Charis SIL}
  \usepackage[style=apa, backend=biber]{biblatex}
    \addbibresource{References.bib}
  \usepackage{hyperref}
    \hypersetup{colorlinks=true, allcolors=blue}
  \usepackage[group-minimum-digits=4, group-separator={,}]{siunitx}
  \usepackage{enumitem}

  % Document information
  \title{Using language models for holistic language variety comparisons\footnote{
    Data and code available at \url{https://osf.io/9cjpw/}.
  }}
  \author{Joshua McNeill}
  \date{\today}

  % New commands
  \newcommand{\orth}[1]{$\langle$#1$\rangle$}
  \newcommand{\lexi}[1]{\textit{#1}}
  \newcommand{\gloss}[1]{`#1'}

\begin{document}
  % <<settings_load_scripts, echo = FALSE>>=
  % read_chunk("analysis.R")
  % opts_chunk$set(echo = FALSE,
  %                warning = FALSE,
  %                message = FALSE,
  %                results = "asis",
  %                fig.height = 3,
  %                fig.width = 5)
  % @
  \maketitle
  \section{Introduction}
    \label{sec:intro}
    One of the goals of scientific research is to discover generalizable facts, and work on language variation is no different.
    Variationists are often interested in describing language varieties and distinguishing between language varieties.
    This task is typically accomplished by analyzing several salient linguistic variables in fine detail and either generalizing to whole varieties from these several variables.
    The focus on deeply analyzing a small number of variables has advanced the understanding of language variation in more ways than can be mentioned here.
    While this paradigm shift from the broad analyses of many variables done dialectology, out of which language variation developed, has been valuable, modern technological advances have made it possible to once again consider analyzing broad swaths of linguistic variables at once.
    The objective for this study is thus to develop a method for holistically quantifying the distance between language varieties in a way that can account for practically all potential linguistic variables.

    Some work on language variation has to already returned to its dialectological roots at least in spirit, and so a look at what has been done recently will help situate the method being proposed here.
    As such, section \ref{sec:intro_saliency} will discuss some advantages and disadvantages of traditional methods of analyzing language variation to show what can be gained from a more holistic approach, and section \ref{sec:intro_clusters} will review the limits of how holistic such traditional studies tend to get.
    Section \ref{sec:intro_nlp} will explain a bit of what is done in the field of natural language processing (NLP), and section \ref{sec:intro_nlpimpact} will review how such technological advances have been applied to variationist studies, leading to the inclusion of many more variables than was previously practical.

    \subsection{Generalizing from saliency}
      \label{sec:intro_saliency}
      In traditional variationist work, generalizations about whole varieties are sometimes drawn from a relatively small set of linguistic variables, perhaps as few as five.
      The assumption underlying that makes this possible is that the variables analyzed are salient and so have social meaning and so are what really distinguish varieties.
      This connection between saliency and social meaning has indeed been acknowledged in variationist literature \parencite[p.~235]{podesva_salience_2011} and has at least been implicit in variationist theory since its foundations.

      \textcite{labov_sociolinguistic_1972} himself introduced the idea of saliency into language variation in proposing the concepts of linguistic variables being either indicators, markers, or stereotypes, the latter two carrying saliency.
      It was later claimed that most linguistic variables are markers \parencite{bell_language_1984}, carrying saliency but not being explicitly commented on by speakers when describing one variety versus another.
      The importance of saliency for classifying types of linguistic variables was also part of the concept of orders of indexicality \parencite{silverstein_indexical_2003}, where second and third order indexicals would be somewhat equivalent to markers and stereotypes, respectively, in terms of saliency.

      Saliency has not only been a consistent aspect of variationist theorizing, but it has proved useful for analyses.
      One example is in analyzing how varieties comes to be social constructs, recognized as existing in the minds of speakers, which has been referred to in language variation as enregisterment \parencite{agha_social_2003}.
      In essence, the process of enregisterment is based on features increasing in saliency.
      This was initially applied to Received Pronunciation \parencite{agha_social_2003} but has also been applied to the development of Pittsburghese based mostly on an analysis of (aw) monophthongization, though some other features are discussed also such as the term \lexi{yinz} \gloss{you (pl.)} \parencite{johnstone_mobility_2006}.
      Additionally, the saliency has been used to distinguish varieties spoken by different groups within a single geographic area in that residents will explicitly describe linguistic features that they associate with each group.
      This was the case in a study of Beijing where two types of Beijingers and one type of non-Beijinger were associated with one linguistic variable each \parencite{zhang_chinese_2005}.\footnote{
        Technically, one group, the ``smooth operators'', was associated with two variables, but each variable involved [ɹ] \parencite[pp.~441-444]{zhang_chinese_2005}.
      }

      The sort of explicit commentary noted in \textcite{zhang_chinese_2005} is one way to establish whether a linguistic variable is salient or not.
      To quantify such commentary requires developing indices where perhaps certain types of commentary each count in the index or commentary in general is counts as just one item among other types in the index.
      In fact, saliency has sometimes even been quantified on purely linguistic grounds \parencite[p.~237]{podesva_salience_2011}.\footnote{
        \textcite{podesva_salience_2011} does of course acknowledge social saliency, as well.
      } Perhaps a more common way of identifying saliency has come through experimentation, however.
      Perceptual studies that invovle experimentation fall under paradigms such as eye-tracking \parencite[e.g.,][]{donofrio_persona-based_2015} or matched-guise experiments \parencite[e.g.,][]{campbell-kibler_nature_2009, delforge_nobody_2012}.
      Findings in such studies include showing that preconceptions of Valley Girls and Californians can impact perceptions of the TRAP vowel \parencite{donofrio_persona-based_2015}, that the alveolar variant of (ing) is judged differently depending on the listener \parencite{campbell-kibler_nature_2009}, and that vowel devoicing in Andean Spanish is less noticeable by listeners when they do not want to notice it, as those who denied having the feature still had it at similar rates to those from elsewhere in whom the feature was regularly perceived \parencite{delforge_nobody_2012}.

      The point is that saliency does indeed exist, can be measured, and carries import for analyses, particularly if one is interested in social meaning.
      To the extent that varieties are social constructs, it is also a rather safe assumption to say that salient variables are what distinguish varieties as that which is not socially meaningful could not be a defining feature of a social construct. % Can I get a reference for this?
      However, when looking at varieties more as concrete systems that are coherent, consistent, and objectively different from each other, comparing salienct variables alone may miss even a large number of distinctions that a more holistic comparison could capture.
      Furthermore, being able to compare between varieties both holistically and by looking at only salient variables opens the possibility of finding contrasts that may be enlightening, such as the possibility that two varieties different in many more ways than what is noticeable to speakers, yet the focus in variationist work has mainly been on salient variables alone.

    \subsection{Looking at clusters of linguistic variables}
      \label{sec:intro_clusters}
      Language variation studies have of course examined clusters of variables and systems of interrelated variables.
      The focus in these cases is still linguistic variables that are thought to be salient, though the number of variables analyzed has at times approached a relatively larger scale.
      This is particularly true when the analysis focuses on ``features'', which are effectively treated as linguistic variables that have two realizations: present or absent.

      Studies of African-American English (AAE)\footnote{
        I am including what researchers may refer to as African-American Vernacular English (AAVE) under this term as the distinction is not particularly important for the present study.
      }, perhaps due simply to there being so many, include both analyses of small clusters of variables but also larger sets of features.
      In some cases, there is an apparent relationship between the variables, such as in \textcite{rickford_addressee-_1994} where the analysis included the zero copula, habitual invariant \lexi{be}, the plural \lexi{-s} morpheme, the 3rd person present \lexi{-s} verbal inflection, and the possessive \lexi{-s} morpheme.
      The first two of these both implicate the verb \lexi{to be} whereas the last three all involve homophonous suffixes.
      Such relationships are not always present, however, as in an examination of Martin Luther King, Jr's language that included the zero copula, (ing), (t/d) deletion, (r), word-final consonant cluster reduction, and prosody \parencite{wolfram_significance_2016}.
      If a structural relationship exists between these linguistic variables, it is not evident at first glance.

      When sets of features from AAE are studied, the number of variables grows substantially.
      In the case of \textcite{van_hofwegen_coming_2010}, 32 features were tallied in individuals' speech as a way to measure how heavily AAE their speech was.
      The measure itself, along with the chosen features, was developed in research on speech pathology and education and where it is referred to as the dialect density measure (DDM).
      While the proposal of an appellation such as DDM suggests something of a novel approach, similar measures have in fact been used elsewhere, as well.
      In a study looking into the use of AAE features in gay British men on Twitter, 25 AAE features were analyzed according to their frequencies \parencite{ilbury_sassy_2020}.
      Also involving Twitter, over 30 non-standard spellings thought to be associated with AAE were analyzed in order to draw AAE dialect maps in the US \parencite{jones_toward_2015}.
      While these feature-conceptualized studies include far more than the typical five to seven clustered variables, they still fall well short of including all possible variables in these varieties.

      If studies of AAE sometimes include clusters of linguistic variables that appear structurally related, it is not always clear that this structural relationship is what drove the choice of variables.
      In many other cases, there is little question that the researchers are interested in a cluster of variables because of their structural relationship.
      This is particularly evident in examinations of vowel systems or large portions of vowel systems, where there is often though not always a chain shift involved.
      This has been done since the foundational variationist work in New York City \parencite{labov_social_2006} and has continued in studies of other areas such as Belfast \parencite{milroy_language_1987}, King of Prussia \parencite{payne_factors_1980}, Syndey \parencite{horvath_delimiting_1987}, the Detroit area \parencite{eckert_linguistic_2000}, and Philadelphia \parencite{eckert_phonetics_2017}.
      Similarly, though without implicating a chain shift, the majority of the pronoun system of Louisiana French as spoken in Houma has been analyzed as a whole \parencite{rottet_language_1995} as well as chunks of the verbal morphology system and and copula system in Cajun English \parencite{dubois_verbal_2003}.
      Finally, perhaps one of the most explicit treatments of a system of covariation was \citeauthor{guy_cognitive_2013}'s (\citeyear{guy_cognitive_2013}) analysis of Brazilian Portuguese as spoken in Rio de Janeiro which had the explicit goal of examining the existence or non-existence of distinctive sociolects by determining if four linguistic variables covaried.
      While these sort of studies go further towards covering all the variables of particular linguistic systems, there is once again much left to be described if one wishes to compare the distance between varieties in their entireties.
      Furthermore, not all subsystems of varieties are contained in such a relatively small number of variables.
      For instance, it is not possible the lexical system of a variety with five, ten, or even one hundred variables.

    \subsection{Advances in NLP}
      \label{sec:intro_nlp}
      A significant part of what has made it possible to analyze much larger numbers of linguistic variables at once is the continued development of NLP technology.
      Indeed, not only has development in NLP become more and more active, NLP has also made its way into other fields. This includes sociolinguistics, where the resulting interdisciplinary field has become known as computational sociolinguistics \parencite{nguyen_computational_2016}.
      While a thorough discussion of NLP is beyond the scope of the present study, a brief overview of the basics of what can be done and how is relevant to make the holistic method of comparing varieties proposed here more understandable.
      NLP has certainly found its way into language variation, but not every variationist is a computational sociolinguist.

      The building of large corpora is not purely an activity related to NLP, but they are central to the development of NLP tools and sizes have exploded over the last decade.
      Sociolinguistic interviews themselves yield corpora, of course, and the first corpus to be over one million words of written English, the Brown corpus, was collected already in the 1960s \parencite{francis_brown_1964}.
      Efforts to digitize written language have continued through the present day with corpora such as the British National Corpus (BNC) consisting over just under 100 million words \parencite{burnard_reference_2007}, the Corpus of Contemporary American English (COCA) consisting of over 400 million words at its start \parencite{davies_corpus_2010}, and Google Books consisting of over 40 million books \parencite{lee_15_2019}.

      In addition to digitized written language, data mining techniques have made collecting large contemporary corpora from social media platforms so prevalent that there is more of a question about the ethics behind such activities than about how to accomplish the task \parencite{darcy_ethics_2012}.
      Corpora from Twitter, for instance, have reached sizes such as 62 million tweets \parencite[p.~519]{hong_language_2011} or 114 million tweets \parencite[p.~13]{eisenstein_phonological_2013} or even 924 million tweets \parencite[p.~244]{huang_understanding_2016}, each tweet containing up to 140 characters\footnote{
        The character limit for tweets was subsequently raised to 280 characters \parencite{rosen_tweeting_nodate}.
      }. Depending on the scope of the sort of data the researcher wishes to collect, such corpora can be constructed in relatively little time, as well, with even the largest example given taking only a year to finish.

      The extent to which such large corpora present opportunities for linguistic research is limited by the tools available to deal with so much data.
      NLP provides a number of tools that help expedite the sort of analyses that variationists might carry out, including part of speech tagging, sentence parsing, and document classification.
        % Part-of-speech tagging is a tool that is now widely available
          % This typically involves training a tagger on some documents that were hand-coded for part-of-speech
          % Luckily, there are numerous corpora that are already hand-coded that can be used at least for major languages, such as the Brown corpus for English (Kuc̆era & Francis 1967)
        % Related to part-of-speech tagging is the task of parsing sentences
          % This is done in much the same way as part-of-speech tagging where some training data is used to train a parser that can then automatically parse new sentences
          % Again, luckily, there are already many corpora that have already been parsed by hand, referred to as treebanks, such as the Penn Treebank for English (Prasad et al. 2019)
          % These tools are useful for variationists looking at syntax but also potentially any other variationist whose linguistic variables may be syntactically conditioned
        % Classifiers are trained to accomplish a number of tasks that are useful for coding documents
          % "Documents" are understood in this case to be any amount of text that acts like a unit, be it a tweet or a book
          % A classic example of a classifier task is identifying the topic of a document
            % The goal is often solving problems such as spotting email spam (Dada et al. 2019), news topic classification, opinion mining, and so on (Minaee et al. 2021)
          % One classification task that is relevant here: language variety identification
            % The goal is to identify the variety used in a document when those varieties are relatively similar
            % This has been worked on for Arabic varieties (Bouamor et al. 2019), Balkan languages, Indonesian vs Malaysian, Portuguese varieties, Spanish varieties, French varieties, and Persian vs Dari (Zampieri et al. 2017)
        % These tools help variationists relegate much work that used to be done by hand to a machine
          % Admittedly, it is still prudent to go through samples of the data for quality assurance, but doing so is much quicker than coding by hand
          % Perhaps the biggest yet simplest advancement, particularly when considering how many variables can be analyzed at once, has just been counting
            % A few lines of basic code can return counts for tokens of linguistic variables from millions to even billions of words of text

    \subsection{The impact of NLP on language variation research}
      \label{sec:intro_nlpimpact}
    % Variationist studies that have looked at large numbers of linguistic variables at once
      % Somewhat between the a very large number of linguistic variables and the more traditional number of variables is Grieve's work
        % For example, Grieve et al. (2018) looked at 54 lexical items on Twitter that they judged to be innovations and mapped their geographic diffusion over time
      % Huang et al. (2016) looked at 211 sets of lexical items to examine regional variation throughout the US as expressed on Twitter
      % Pavalanathan & Eisenstein (2015) looked at over 200 lexical variables, treating them as binary options of either being present or absent, to examine the influence of audience on language use on Twitter
      % Bamman et al. (2014) looked at 10,000 lexical items in their analysis of gender on Twitter, though they used these items a bit differently from how they would be used in a traditional variationist study
        % The lexical items were first used to train a classifier to identify the gender of Twitter users
        % They then looked at which words were most strongly associated with one gender or the other, which is where the analysis more resembles traditional studies
        % Their methods are not unlike those that will be proposed here in that they also include a marriage of NLP and sociolinguistics
      % In some ways, these studies harken back to the early days of dialectology where the goal was to produce Atlases that demonstrated the boundaries between regional varieties such as the Atlas linguistique de la France for French (Gilliéron & Edmond 1902) or the Sprach-Atlas for German (Wenker 1881/2020)
        % New technologies give modern work the opportunities for insights that were not possible in the past
        % A similar leap in research possibilities based on technological advancement that has been noted is the availability of tape recorders, which made socioilnguistic research based on real speech starting in the 1960s a reality (Milroy & Milroy 1985:52)

    \subsection{Generalizing from $n$-gram language models}
      \label{sec:intro_lmproposal}
    % These NLP-advantaged studies are fairly holistic, but are they holistic enough?
      % A lot of these studies are really only looking at lexical variation even though they include many variables
      % A more holistic approach would include more than individual lexical items, even if very many individual lexical items are considered
      % How are other linguistic levels captured?
    % An even more holistic approach is to use character n-gram and word n-gram language models
      % n-gram language models take every unique n number of consecutive symbols in a corpus and counts up how often they occur
        % The symbols typically used are words or characters, but any symbol can be used
          % Given a corpus tagged for part-of-speech, consecutive parts-of-speech may be used, for instance
          % Given a corpus fully transcribed into IPA, consecutive IPA symbols may be used
        % An example might be a word bigram model
          % For a sentence such as "this is a sentence", the word bigrams contained would be "this is", "is a", and "a sentence"
          % Alternatively, a character 4-gram of the same sentence would yield "this, "his ", "is i", "s is", " is ", and so on
      % n-gram language models have been used in classifiers in NLP for the task of language identification and language variety identification, as mentioned above
      % The success of these models in distinguishing between even minorly different language varieties suggests that they can be compared to quantify the linguistic distance between two varieties, as well
        % Language models in NLP are, after all, essentially just probability distributions over some set of symbols
    % Research question
      % Are n-gram language models useful for quantifying the linguistic distance between varieties in a more holistic way than has been previously possible?

  \section{Methods}
    \label{sec:meths}
  % Methods
    % Data collection
      % The europarl corpus
        % This is a parallel corpus that can be used to set the baseline for what one should expect between what are generally viewed as completely different languages and between one variety at different times
          % For different languages: built models from the English and French documents
          % For one variety: built models from the first and second halves of the English documents
          % In effect, this data can be said to be coded for language and text section
      % [Twitter data will be left out for the QP1 version] Twitter data
        % Characteristics of Twitter
          % What makes this appropriate for this study
        % Tweets collected from Louisiana and filtered for discussion of race and ethnicity
          % Narrowing the type of data analyzed helps ensure that distinctions between communities won't be the result of simply different topics of conversation
          % The race/ethnicity situation in Louisiana has been analyzed before and is complex, so narrowing in this way may provide insights into social questions, though that's not the primary objective of this study
        % Coding
          % The goal here is to compare linguistic distance between communities
          % Community detection
            % Communities are conceptualized on structural grounds in that those who interact with the same people on a consistent basis are considered to form a community
              % In this case, a directed tweet counts as the sender interacting with the receiver
              % The number of directed tweets between two users counts as the amount of interaction
            % Blondel et al.'s community detection algorithm is used
          % Language models will be built for a handful of communities which contain substantial numbers of tweets about race/ethnicity
            % Race/ethnicity are chosen because Louisiana has a complex history in this regard
              % Historically was not divided by Black and White, instead having Black, Creole, and White, but has come to be Black=Creole vs White=Cajun today
            % Race/ethnicity tweets will be detected using a classifier trained on 1% of the total tweets collected (~40,000)
            % There is some subjectivity about what exactly constitutes discussion of race/ethnicity
              % Include some examples of tweets that are unarguably about race/ethnicity and some that are not
    % Language models
      % Following Duvenhage (2019) due to his success for DSL 17, both word and character n-grams were trained, unigrams and bigrams for the former, bigrams, 4-grams, and 6-grams for the latter.
        % These models each capture different levels of the language, so I conceptualize them as follows:
          % Word unigrams: lexicon
          % Word bigrams: syntax (or at least some small amount of syntax)
          % Character n-grams: orthography and morphology
    % Analysis
      % Two statistics are used: KL divergence and cosine similarity
        % Both of these show essentially the same thing (i.e., how similar/different two distributions are), but the former is easier to interpret and the latter makes comparisons between language pairs easier as values are always between -1 and 1
        % These are calculated for like-models (e.g., word unigram to word unigram, character 4-gram to character 4-gram, etc.)
          % This makes it possible to see how distant two varieties are on different linguistic levels

  \section{Results}
    \label{sec:results}
  % Results
    % europarl baseline results
      % What are the KL divergence and cosine similarity values when comparing French and English?
        % What about when comparing different model types? (Look at combining all models and looking at models individually)
      % What are the KL divergence and cosine similarity values when comparing English-English comparisons and French-French comparisons?
        % What about when comparing different model types? (Look at combining all models and looking at models individually)
    % [After finishing the QP1 version of this,] do the same for the Twitter data between detected communities

  \section{Discussion}
    \label{sec:discuss}
  % Discussion
    % This technique for comparing varieties holistically appears to work, but there are some limitations to consider and questions left unanswered
      % Are the language models used here the right models?
      % How big do corpora need to be in order for this technique to work?
      % How does this relate to traditional work that typically examines only a few linguistic variables but in great detail?
    % Are the models used truly the best language models that could be used here?
      % For instance, KL divergence here is calculated based on the empirical probabilities for each n-gram in each model (i.e., relative frequencies)
      % An alternative would be to use something like term frequency minus inverse document frequency (TF-IDF), which is commonly used in NLP and is meant to give the greatest weight in a model to n-grams that are frequent in a small number of documents
        % Eisenstein (2014) took a similar approach when identifying terms of import for differentiating between dialects on Twitter (5)
      % If, however, the type of models used here are not optimal, the approach can still be just as easily applied to other types of language models, both existing and yet to be proposed types
    % Related to whether these are the best types of language models to use for this technique of comparing varieties is the question of how large the corpora that are used to generate the language models ought to be
      % The europarl corpora are quite large, which presumably means there would not be a great difference between something like an empirical probability and any other sort of calculated probability
      % There are, however, cases where a researcher might want to apply the technique when using much smaller corpora
      % There may very well be a threshold where different language model types are more or less useful depending on whether the corpora size is above or below the threshold
        % This is something that cannot be addressed in the present study, but would be worthwhile to examine in future work
    % As was discussed earlier, salient linguistic variables may be far more important than other variables
      % For instance, the technique used here might find two varieties differ in that one variety uses the preposition "for" where another uses "of" sometimes
        % It is not evident that such a small difference is noticeable at all, even subconsciously, by speakers of these varieties
      % If the differences found between two varieties are not salient on any level, does it actually make sense to speak of them as two separate varieties?
        % This question cannot be answered here, but it is at least a question that can now be examined by comparing the results from a holistic technique as used here to results for the same data from a more traditional, fine-grained analysis

  \section{Conclusion}
    \label{sec:conc}

  \printbibliography
\end{document}
